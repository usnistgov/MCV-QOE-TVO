---
title: "Transmit Volume Optimization Verification"
author: "William Magrogan"
date: "10/27/2020"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r disclaimer, echo=FALSE}
# This software was developed by employees of the National Institute of Standards and Technology (NIST), an agency of the Federal Government. Pursuant to title 17 United States Code Section 105, works of NIST employees are not subject to copyright protection in the United States and are considered to be in the public domain. Permission to freely use, copy, modify, and distribute this software and its documentation without fee is hereby granted, provided that this notice and disclaimer of warranty appears in all copies.
# 
# THE SOFTWARE IS PROVIDED 'AS IS' WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND FREEDOM FROM INFRINGEMENT, AND ANY WARRANTY THAT THE DOCUMENTATION WILL CONFORM TO THE SOFTWARE, OR ANY WARRANTY THAT THE SOFTWARE WILL BE ERROR FREE. IN NO EVENT SHALL NIST BE LIABLE FOR ANY DAMAGES, INCLUDING, BUT NOT LIMITED TO, DIRECT, INDIRECT, SPECIAL OR CONSEQUENTIAL DAMAGES, ARISING OUT OF, RESULTING FROM, OR IN ANY WAY CONNECTED WITH THIS SOFTWARE, WHETHER OR NOT BASED UPON WARRANTY, CONTRACT, TORT, OR OTHERWISE, WHETHER OR NOT INJURY WAS SUSTAINED BY PERSONS OR PROPERTY OR OTHERWISE, AND WHETHER OR NOT LOSS WAS SUSTAINED FROM, OR AROSE OUT OF THE RESULTS OF, OR USE OF, THE SOFTWARE OR SERVICES PROVIDED HEREUNDER.
#
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In order to use this code, update the PATH_TO_DATA variable to contain the path to your local data copy.


```{r user-input-path,echo=FALSE, message=FALSE}
PATH_TO_DATA <- "...directory/Pub_Data/Final Data/"
```


```{r path-definitions,echo=FALSE, message=FALSE}
p25_direct_test_files <- list.files(
  path=file.path(PATH_TO_DATA, "P25 Direct"), pattern="\\.csv",
  full.names=TRUE
  )
p25_phase_2_test_files <- list.files(
  path=file.path(PATH_TO_DATA, "P25 Phase 2"), pattern="\\.csv",
  full.names=TRUE
  )
analog_direct_test_files <- list.files(
  path=file.path(PATH_TO_DATA, "Analog Direct"), pattern="\\.csv", 
  full.names=TRUE, recursive=TRUE
  )
```


```{r, echo = FALSE, warning = FALSE, message=FALSE, include=FALSE}
# Required packages
packages <- c(
  "ggpubr",
  "dplyr",
  "ggplot2",
  "broom"
)

# Install any packages that need it
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
  install.packages(setdiff(packages, rownames(installed.packages())))
}
# Bring required packages into library
lapply(
  packages,
  FUN = function(packages) {
    do.call("require", list(packages))
  }
)

options(dplyr.summarise.inform=FALSE)

# setup auxiliary functions
read_volume_csv <- function(x){
  bounds <- lapply(x, function(y) read.csv(y, nrows=1))
  bounds <- do.call(rbind, bounds)
  raw_data <- lapply(x, function(y) read.csv(y, skip=2))
  raw_data <- do.call(rbind, raw_data)
  return(list(data.frame(bounds), data.frame(raw_data)))
}

# propagate uncertainty through linear interpolation
prop_unc_lerp <- function(x_data, unc_data, eval_point) {
  min_index <- which.min(abs(eval_point-x_data))
  if(eval_point - x_data[min_index] < 0) {
    i_upper <- min_index
    i_lower <- min_index - 1
  }
  else {
    i_lower <- min_index
    i_upper <- min_index + 1
  }
  
  # Do the lerp 
  lerp_var <- ((eval_point - x_data[i_upper])^2*unc_data[i_lower]^2 + 
                 (eval_point - x_data[i_lower])^2*unc_data[i_upper]^2)/(x_data[i_upper] - x_data[i_lower])^2

    # return lerp uncertainty
  lerp_unc <- sqrt(lerp_var)
  return(lerp_unc)
}
```


This document examines success metrics for the transmit volume optimization (TVO) methods developed.

# P25 Direct

## Metric 1: Verify with PESQ

Below is a table detailing the optimum volume levels as determined by the TVO and their corresponding FSF and PESQ scores.
```{r, echo = FALSE, warning = FALSE, message=FALSE}
test_files <- p25_direct_test_files

# create dataframe for populating
ov <- c()
fs <- c()
ps <- c()
fmd <- c()
pmd <- c()
fmv <- c()
pmv <- c()
fvar <- c()
pvar <- c()

pesq_opt_vol_unc <- c()
pesq_max_vol_unc <- c()

# get relevant data from each file
for(fn in test_files) {
  raw_data <- read_volume_csv(fn)
  opt_data <- raw_data[[1]]
  crv_data <- raw_data[[2]]
  proc_data <- summarise(
    group_by(crv_data, Volume_levels..dB.),
    FSF_avg=mean(FSF),
    PESQ_avg=mean(PESQ),
    FSF_var=var(FSF),
    PESQ_var=var(PESQ)
    )
  
  # interpolating polynomials for evaluating FSF and PESQ, linear interp
  faf <- approxfun(proc_data$Volume_levels..dB., proc_data$FSF_avg)
  paf <- approxfun(proc_data$Volume_levels..dB., proc_data$PESQ_avg)
  
  # add data
  ov <- c(ov, opt_data$Optimum)
  fs <- c(fs, faf(opt_data$Optimum))
  ps <- c(ps, paf(opt_data$Optimum))
  fmd <- c(fmd, max(proc_data$FSF_avg)-faf(opt_data$Optimum))
  pmd <- c(pmd, max(proc_data$PESQ_avg)-paf(opt_data$Optimum))
  fmv <- c(fmv, abs(proc_data[which.max(proc_data$FSF_avg),]$Volume_levels..dB. - ov[length(ov)]))
  pmv <- c(pmv, abs(proc_data[which.max(proc_data$PESQ_avg),]$Volume_levels..dB. - ov[length(ov)]))
  
  # propagate uncertainty through linear interpolation
  pesq_opt_vol_unc <- c(
    pesq_opt_vol_unc,
    prop_unc_lerp(proc_data$Volume_levels..dB., sqrt(proc_data$PESQ_var), opt_data$Optimum)
    )
  # get PESQ max volume level
  pesq_max_vol_level <- which.max(proc_data$PESQ_avg)
  pesq_max_vol_unc <- c(pesq_max_vol_unc, sqrt(proc_data[pesq_max_vol_level,]$PESQ_var))
}

comp_table <- data.frame(
  "Opt Volume"=ov,
  "FSF"=fs,
  "PESQ"=ps,
  "FSF Max Diff"=fmd,
  "PESQ Max Diff"=pmd,
  "FSF Max Vol Diff"=fmv,
  "PESQ Max Vol Diff"=pmv
)

# A few summary stats
df <- rbind(
  colMeans(comp_table),
  data.frame(lapply(comp_table, function(x) sqrt(var(x)/length(x))))
)

# Summary stats (min, max, mean, SE) for PESQ Max Diff column
PESQ_Max_Diff_stats_P25_Direct <- c(
  "Min" = min(pmd),
  "Max" = max(pmd),
  "Mean" = mean(pmd),
  "SE" = 0.0034300,
  "Lower" = mean(pmd) - 2*0.0034300,
  "Upper" = mean(pmd) + 2*0.0034300
 )

# delta represents the uncertainty due to measurement resolution. Taken as the 
# minimum difference between two measurements.

# delta

df[["Opt.Volume"]][[2]] <-  sqrt(df[["Opt.Volume"]][[2]]^2)
rownames(df) <- c("Mean", "SE")

headers <- c(
    "Opt Volume (dB)",
    "FSF",
    "PESQ",
    "FSF Max Diff",
    "PESQ Max Diff",
    "FSF Vol Diff (dB)",
    "PESQ Vol Diff (dB)"
    )

# make the table
knitr::kable(
  comp_table,
  col.names=headers
  )

knitr::kable(
  df,
  col.names=headers
  )


```

To explain the columns:

* Opt Volume: Describes the output the transmit volume optimization (TVO) script describes as optimal
* FSF/PESQ: The respective score evaluated at the optimal volume. A linear
interpolation is used since it is unlikely that score was evaluated at the
optimum volume while the script was collecting data.
* FSF/PESQ Max Diff: This describes the difference between the observed maximum
(determined from the evaluated points) and the respective score evaluated at the
prescribed optimum volume.
* FSF/PESQ Vol Diff: Describes the distance (in absolute value) between the
optimum volume and the location of the observed maximum, perhaps more simply
put, it is abs($v_{\mathrm{max}} - v_{\mathrm{optimum}}$) where
$\mathrm{score}(v_{\mathrm{max}}) = \mathrm{score}_{max}$

A few notes on the second table:

* The uncertainty due to the discrete nature of
the volume adjust algorithm was incorporated into the standard error of the
optimum volume 
* The difference between FSF observed max and the opt volume is on the order of 
it's standard error, and hence likely due to noise.
* The difference between PESQ observed max and the opt volume is greater than 
it's standard error, and may be significant, but the difference is well within 
1% error of the max, so it is likely not very impactful

The table below gives the full uncertainty 
```{r, echo = FALSE, warning = FALSE, message=FALSE}
P25Direct_t1 <- data.frame(
    "Opt_PESQ" = c(df$PESQ[[1]], sqrt(mean(pesq_opt_vol_unc^2))),
    "Max_PESQ" = c(df$PESQ[[1]] + df[["PESQ.Max.Diff"]][[1]], sqrt(mean(pesq_max_vol_unc^2)))
    )
row.names(P25Direct_t1) <- c("Mean", "Standard Error")
knitr::kable(
  P25Direct_t1
  )

```



## Metrics 2 and 3: Optimal interval and volume stability

Here we look at the stability of the optimal values and intervals across trials.
```{r, echo = FALSE, warning = FALSE, message=FALSE, fig.align='center'}
ov <- c()
lb <- c()
ub <- c()

for(fn in test_files) {
  raw_data <- read_volume_csv(fn)
  opt_data <- raw_data[[1]]
  crv_data <- raw_data[[2]]
  proc_data <- summarise(
    group_by(
      crv_data,
      Volume_levels..dB.
      ),
    FSF_avg=mean(FSF),
    PESQ_avg=mean(PESQ),
    )

  ov <- c(ov, opt_data$Optimum)
  lb <- c(lb, opt_data$Lower_Interval)
  ub <- c(ub, opt_data$Upper_Interval)
}

# save data for later
p25_direct_lb <- lb
p25_direct_ub <- ub

opt_vals <- data.frame(
  "Optimum"=ov,
  "Lower_Bound"=lb, 
  "Upper_Bound"=ub,
  "Interval_Width"=(ub - lb)
)

ivrhist <- ggplot(opt_vals, aes(`Interval_Width`)) +
  geom_histogram(binwidth=.1) +
  xlab("Interval Width [dB]") +
  ylab("Counts") +
  ggtitle("Interval Width Distribution")

opthist <- ggplot(opt_vals, aes(`Optimum`)) +
  geom_histogram(binwidth=.1, fill="blue") +
  xlab("Optimum Volume [dB]") +
  ylab("Counts") +
  ggtitle("P25 Direct") + 
  theme(axis.text.x = element_text(face = "bold", size = 10)
  )

lbhist <- ggplot(opt_vals, aes(`Lower_Bound`)) +
  geom_histogram(binwidth=.1) +
  xlab("Lower Bound [dB]") +
  ylab("Counts") +
  ggtitle("Lower Bound Distribution")


ubhist <- ggplot(opt_vals, aes(`Upper_Bound`)) +
  geom_histogram(binwidth=.1) +
  xlab("Upper Bound [dB]") +
  ylab("Counts") +
  ggtitle("Upper Bound Distribution")

ggarrange(ivrhist, opthist, lbhist, ubhist, ncol=2, nrow=2)
ggsave("p25_direct_opt.png", plot=opthist, height=5, width=5)
p25_direct_opthist <- opthist
```

Focusing on the data histograms as opposed to the raw summary statistics because it highlights the
discrete nature these numbers which is not captured by the statistics. The
histograms show that the lower bound is consistently -42.2 dB across all 10
trials with no variation. The upper bound bounces back and forth between -13.3 dB
and -11.1 dB almost equally. 
From these the interval endpoints other quantities can be calculated (interval width and opt volume). 

The TVO incurs a search
tolerance of 1 dB. Therefore a type B uncertainty of $
both the upper and lower boundary.

```{r, echo = FALSE, warning = FALSE, message=FALSE}
# uncertainty due to measurement
delta <- 1/sqrt(12)

# Summarize values
a <- summarise(
  opt_vals,
  Lower_Bound_Mean=mean(`Lower_Bound`),
  Lower_Bound_SE=sqrt(var(`Lower_Bound`)/length(`Lower_Bound`) + delta^2),
  Upper_Bound_Mean=mean(`Upper_Bound`),
  Upper_Bound_SE=sqrt(var(`Upper_Bound`)/length(`Upper_Bound`) + delta^2)
  )

knitr::kable(a)
```
\newpage
## Uncertainty on bounds using coverage factor k= 2.26
```{r, echo = FALSE, warning = FALSE, message=FALSE}
k <- 2.26
Lower_Bound_SE <- sqrt(var(lb)/length(lb) + delta^2)
lower_bound_unc <- k * Lower_Bound_SE

Upper_Bound_SE <- sqrt(var(ub)/length(ub) + delta^2)
upper_bound_unc <- k* Upper_Bound_SE  
  
unc_bounds <-data.frame(lower_bound_unc, upper_bound_unc)

knitr::kable(unc_bounds, "pipe", col.names = c("Lower Bound Unc", "Upper Bound Unc"), align = c("l", "c", "c"))

```

## Weighting Analysis

This process runs multiple definitions for the optimum volume. It is desired to determine a
weighting factor that has PESQ scores close to the maximum within the interval selected by the upper and lower volume boundaries.

```{r P25 Direct Interval Comparison, fig.height=11, fig.width=8, echo=FALSE, warning=FALSE}
# First: look at the range of weights to look at
weights <- seq(.5, .9, by=.01)
best_weights <- c()
best_significant_weight <- c()
stat_diff_weight <- c()
devs <- c()
plt_list <- list()

# grab data from only one of the tests for P25 direct
for(fn in test_files) {
  raw_data <- read_volume_csv(fn)
  opt_data <- raw_data[[1]]
  crv_data <- raw_data[[2]]
  proc_data <- summarise(
    group_by(
      crv_data,
      Volume_levels..dB.
      ),
    FSF_avg=mean(FSF),
    PESQ_avg=mean(PESQ),
    FSF_var=var(FSF),
    PESQ_var=var(PESQ)
    )
  
  paf <- approxfun(proc_data$Volume_levels..dB., proc_data$PESQ_avg)
  pesq_max <- max(proc_data$PESQ_avg)
  
  # propagate uncertainty through linear interpolation
  pesq_opt_vol_unc <- prop_unc_lerp(proc_data$Volume_levels..dB., sqrt(proc_data$PESQ_var), opt_data$Optimum)
  
  # get PESQ max volume level
  pesq_max_vol_level <- which.max(proc_data$PESQ_avg)
  pesq_max_vol_unc <- sqrt(proc_data[pesq_max_vol_level,]$PESQ_var)
  
  lb <- opt_data$Lower_Interval
  ub <- opt_data$Upper_Interval
  
  # figure out their corresponding optimal volumes
  optimum_volumes <- lb * (1-weights) + ub * weights
  
  # get PESQ scores at the newly made optimum volumes
  pesq_at_max_volume <- paf(optimum_volumes)

  # get deviations from max observed pesq
  deviations <- pesq_max - pesq_at_max_volume
  
  # standard deviation at each point
  sds <- sapply(optimum_volumes, function(x) prop_unc_lerp(proc_data$Volume_levels..dB., sqrt(proc_data$PESQ_var), x))
  sds <- sqrt(sds^2+pesq_max_vol_unc^2)/sqrt(40)
  
  # get best weight
  best_weights <- c(best_weights, weights[which.min(deviations)])
  
  # bind observations
  devs <- rbind(devs, deviations)

  # Find non-overlapping confidence interval from best weight
  min_dev_i <- which.min(deviations)
  tmp <- deviations - 1.96*sds - deviations[min_dev_i] - 1.96*sds[min_dev_i]
  ii <- which(tmp > 0)[1]
  if(is.na(ii)){
    best_significant_weight <- c(best_significant_weight, 1)
  } else {
    best_significant_weight <- c(best_significant_weight, weights[ii-1])
  }
  
  # Different from zero?
  sig_diff <- (deviations > 1.96*sds) | (deviations < -1.96*sds)
  if(any(sig_diff)){
    stat_diff_weight <- c(stat_diff_weight, weights[min(which(sig_diff))])
  } else {
    stat_diff_weight <- c(stat_diff_weight, 1)
  }
 
  # plot
  df <- data.frame(
    opt_vols=optimum_volumes,
    devs=deviations,
    upper=deviations+1.96*sds,
    lower=deviations-1.96*sds
  )
  f <- ggplot(df) +
    geom_point(aes(opt_vols, devs), color="blue")+
    geom_point(aes(opt_vols, upper), color="red")+
    geom_point(aes(opt_vols, lower), color="red")+
    ggtitle(paste("Test: ", fn))+
    xlab("Selected Optimum Volume (dB)")+
    ylab("PESQ Deviation from Max")
  
  #show(f)
  plt_list[[fn]] <- f
  
}
ggarrange(plotlist=plt_list, ncol=2, nrow=5)
print(paste("Average Best Weight is:", mean(best_weights)))
print(paste("Best significant weight is:", mean(best_significant_weight)))
print(paste("Mean stat diff weight is:", mean(stat_diff_weight)))


```

### Discussion

These plots represent the deviations of the PESQ score at a
proposed optimum volume (the x-axis) from the maximum observed PESQ score in the
data. The middle line in blue is the mean value surrounded in red by its
confidence interval ($\pm 1.96$ * standard error). PESQ isn't necessarily evaluated at the
optimum volume, so a linear interpolation of nearest values is required. There
are two metrics in terms of the weighting factor, 'Average Best
Weight' and 'Best Significant Weight'. The former indicates the weight that
produces the smallest PESQ deviation from maximum averaged over all the tests
(in this case 10 tests, in the case of analog direct, 30). The latter searches
for the largest volume for which the PESQ score is statistically equivalent to
the minimum deviation PESQ score. In this technology, the results are ~0.571 and
1 respectively. This means that the weight 0.571 will on average get 
closest to the minimum deviation PESQ score. And the second number implies that
we can use weight 1 to be statistically equivalent to the best PESQ score, read
as: we can use the upper bound with no meaningful speech quality drop.

# Analog Direct

## Metric 1: Verify with PESQ

```{r, echo = FALSE, warning = FALSE, message=FALSE}
test_files <- analog_direct_test_files

# create dataframe for populating
ov <- c()
fs <- c()
ps <- c()
fmd <- c()
pmd <- c()
fmv <- c()
pmv <- c()

pesq_opt_vol_unc <- c()
pesq_max_vol_unc <- c()

# get relevant data from each file
for(fn in test_files) {
  raw_data <- read_volume_csv(fn)
  opt_data <- raw_data[[1]]
  crv_data <- raw_data[[2]]
  proc_data <- summarise(
    group_by(crv_data, Volume_levels..dB.),
    FSF_avg=mean(FSF),
    PESQ_avg=mean(PESQ),
    FSF_var=var(FSF),
    PESQ_var=var(PESQ)
    )
    
  # interpolating polynomials for evaluating FSF and PESQ, linear interp
  faf <- approxfun(proc_data$Volume_levels..dB., proc_data$FSF_avg)
  paf <- approxfun(proc_data$Volume_levels..dB., proc_data$PESQ_avg)
  
  # add data
  ov <- c(ov, opt_data$Optimum)
  fs <- c(fs, faf(opt_data$Optimum))
  ps <- c(ps, paf(opt_data$Optimum))
  fmd <- c(fmd, max(proc_data$FSF_avg)-faf(opt_data$Optimum))
  pmd <- c(pmd, max(proc_data$PESQ_avg)-paf(opt_data$Optimum))
  fmv <- c(fmv, abs(proc_data[which.max(proc_data$FSF_avg),]$Volume_levels..dB. - ov[length(ov)]))
  pmv <- c(pmv, abs(proc_data[which.max(proc_data$PESQ_avg),]$Volume_levels..dB. - ov[length(ov)]))
  
  # propagate uncertainty through linear interpolation
  pesq_opt_vol_unc <- c(
    pesq_opt_vol_unc,
    prop_unc_lerp(proc_data$Volume_levels..dB., sqrt(proc_data$PESQ_var), opt_data$Optimum)
    )
  # get PESQ max volume level
  pesq_max_vol_level <- which.max(proc_data$PESQ_avg)
  pesq_max_vol_unc <- c(pesq_max_vol_unc, sqrt(proc_data[pesq_max_vol_level,]$PESQ_var))
}

comp_table <- data.frame(
  "Opt Volume"=ov,
  "FSF"=fs,
  "PESQ"=ps,
  "FSF Max Diff"=fmd,
  "PESQ Max Diff"=pmd,
  "FSF Max Vol Diff"=fmv,
  "PESQ Max Vol Diff"=pmv
)

# A few summary stats
df <- rbind(
  colMeans(comp_table),
  data.frame(lapply(comp_table, function(x) sqrt(var(x)/length(x))))
)

# Summary stats (min, max, mean and SE) for PESQ Max Diff column
PESQ_Max_Diff_stats_Analog_Direct <- c(
  "Min" = min(pmd),
  "Max" = max(pmd),
  "Mean" = mean(pmd),
  "SE" = 0.0013416,
  "Lower" = mean(pmd) - 2*0.0013416,
  "Upper" = mean(pmd) + 2*0.0013416
 )

df[["Opt.Volume"]][[2]] = sqrt(df[["Opt.Volume"]][[2]]^2)
rownames(df) <- c("Mean", "SE")

headers <- c(
    "Opt Volume (dB)",
    "FSF",
    "PESQ",
    "FSF Max Diff",
    "PESQ Max Diff",
    "FSF Vol Diff (dB)",
    "PESQ Vol Diff (dB)"
    )

# make the table
knitr::kable(
  comp_table,
  col.names=headers
  )

knitr::kable(
  df,
  col.names=headers
  )
```

The table below gives the full uncertainty 
```{r, echo = FALSE, warning = FALSE, message=FALSE}
AnalogDirect_t1 <- data.frame(
    "Opt_PESQ" = c(df$PESQ[[1]], sqrt(mean(pesq_opt_vol_unc^2))),
    "Max_PESQ" = c(df$PESQ[[1]] + df[["PESQ.Max.Diff"]][[1]], sqrt(mean(pesq_max_vol_unc^2)))
    )
row.names(AnalogDirect_t1) <- c("Mean", "Standard Error")
knitr::kable(
  AnalogDirect_t1
  )

```

## Metrics 2 and 3: Optimal interval and volume stability

```{r, echo = FALSE, warning = FALSE, message=FALSE, fig.align='center'}
ov <- c()
lb <- c()
ub <- c()

for(fn in test_files) {
  raw_data <- read_volume_csv(fn)
  opt_data <- raw_data[[1]]
  crv_data <- raw_data[[2]]
  proc_data <- summarise(
    group_by(
      crv_data,
      Volume_levels..dB.
      ),
    FSF_avg=mean(FSF),
    PESQ_avg=mean(PESQ),
    )

  ov <- c(ov, opt_data$Optimum)
  lb <- c(lb, opt_data$Lower_Interval)
  ub <- c(ub, opt_data$Upper_Interval)
}

# save data for later
analog_direct_lb <- lb
analog_direct_ub <- ub

opt_vals <- data.frame(
  "Optimum"=ov,
  "Lower_Bound"=lb, 
  "Upper_Bound"=ub,
  "Interval_Width"=(ub - lb)
)

ivrhist <- ggplot(opt_vals, aes(`Interval_Width`)) +
  geom_histogram(binwidth=.1) +
  xlab("Interval Width [dB]") +
  ylab("Counts") +
  ggtitle("Interval Width Distribution")

opthist <- ggplot(opt_vals, aes(`Optimum`)) +
  geom_histogram(binwidth=.1, fill="blue") +
  xlab("Optimum Volume [dB]") +
  ylab("Counts") +
  ggtitle("Analog Direct") + 
  theme(axis.text.x = element_text(face = "bold", size = 10)
  )

lbhist <- ggplot(opt_vals, aes(`Lower_Bound`)) +
  geom_histogram(binwidth=.1) +
  xlab("Lower Bound [dB]") +
  ylab("Counts") +
  ggtitle("Lower Bound Distribution")


ubhist <- ggplot(opt_vals, aes(`Upper_Bound`)) +
  geom_histogram(binwidth=.1) +
  xlab("Upper Bound [dB]") +
  ylab("Counts") +
  ggtitle("Upper Bound Distribution")

ggarrange(ivrhist, opthist, lbhist, ubhist, ncol=2, nrow=2)
ggsave("analog_direct_opt.png", plot=opthist, height=5, width=5)
analog_direct_opthist <- opthist
```

```{r, echo = FALSE, warning = FALSE, message=FALSE}
# uncertainty due to measurement
delta <- 1/sqrt(12)

# Summarize values
a <- summarise(
  opt_vals,
  Lower_Bound_Mean=mean(`Lower_Bound`),
  Lower_Bound_SE=sqrt(var(`Lower_Bound`)/length(`Lower_Bound`) + delta^2),
  Upper_Bound_Mean=mean(`Upper_Bound`),
  Upper_Bound_SE=sqrt(var(`Upper_Bound`)/length(`Upper_Bound`) + delta^2),
  Lower_var=var(`Lower_Bound`) + delta^2,
  Upper_var=var(`Upper_Bound`) + delta^2
  )

knitr::kable(a)
```

## Uncertainty on bounds using coverage factor k= 2.05
```{r, echo = FALSE, warning = FALSE, message=FALSE}
k <- 2.05
Lower_Bound_SE <- sqrt(var(lb)/length(lb) + delta^2)
lower_bound_unc <- k * Lower_Bound_SE

Upper_Bound_SE <- sqrt(var(ub)/length(ub) + delta^2)
upper_bound_unc <- k* Upper_Bound_SE  
  
unc_bounds <-data.frame(lower_bound_unc, upper_bound_unc)

knitr::kable(unc_bounds, "pipe", col.names = c("Lower Bound Unc", "Upper Bound Unc"), align = c("l", "c", "c"))
```

## Weighting Analysis
Weighting analysis for analog direct 

```{r Analog Direct Interval Comparison, fig.height=11, fig.width=8, echo=FALSE, warning=FALSE}

# First thing is first: look at the range of weights to look at
weights <- seq(.5, .9, by=.01)
best_weights <- c()
best_significant_weight <- c()
stat_diff_weight <- c()
devs <- c()
plt_list <- list()

# grab data from only one of the tests for P25 direct
for(fn in test_files) {
  raw_data <- read_volume_csv(fn)
  opt_data <- raw_data[[1]]
  crv_data <- raw_data[[2]]
  proc_data <- summarise(
    group_by(
      crv_data,
      Volume_levels..dB.
      ),
    FSF_avg=mean(FSF),
    PESQ_avg=mean(PESQ),
    FSF_var=var(FSF),
    PESQ_var=var(PESQ)
    )
  
  paf <- approxfun(proc_data$Volume_levels..dB., proc_data$PESQ_avg)
  pesq_max <- max(proc_data$PESQ_avg)
  
  # propagate uncertainty through linear interpolation
  pesq_opt_vol_unc <- prop_unc_lerp(proc_data$Volume_levels..dB., sqrt(proc_data$PESQ_var), opt_data$Optimum)
  
  # get PESQ max volume level
  pesq_max_vol_level <- which.max(proc_data$PESQ_avg)
  pesq_max_vol_unc <- sqrt(proc_data[pesq_max_vol_level,]$PESQ_var)
  
  lb <- opt_data$Lower_Interval
  ub <- opt_data$Upper_Interval
  
  # figure out their corresponding optimal volumes
  optimum_volumes <- lb * (1-weights) + ub * weights
  
  # get PESQ scores at the newly made optimum volumes
  pesq_at_max_volume <- paf(optimum_volumes)

  # get deviations from max observed PESQ
  deviations <- pesq_max - pesq_at_max_volume
  
  # standard deviation at each point
  sds <- sapply(optimum_volumes, function(x) prop_unc_lerp(proc_data$Volume_levels..dB., sqrt(proc_data$PESQ_var), x))
  sds <- sqrt(sds^2+pesq_max_vol_unc^2)/sqrt(40)
  
  # get best weight
  best_weights <- c(best_weights, weights[which.min(deviations)])
  
  # bind observations
  devs <- rbind(devs, deviations)
  
  # Find non-overlapping confidence interval from best weight
  min_dev_i <- which.min(deviations)
  tmp <- deviations - 1.96*sds - deviations[min_dev_i] - 1.96*sds[min_dev_i]
  ii <- which(tmp > 0)[1]
  if(is.na(ii)){
    best_significant_weight <- c(best_significant_weight, 1)
  } else {
    best_significant_weight <- c(best_significant_weight, weights[ii-1])
  }
  
  # Different from zero?
  sig_diff <- (deviations > 1.96*sds) | (deviations < -1.96*sds)
  if(any(sig_diff)){
    stat_diff_weight <- c(stat_diff_weight, weights[min(which(sig_diff))])
  } else {
    stat_diff_weight <- c(stat_diff_weight, 1)
  }
 
  # plot
  df <- data.frame(
    opt_vols=optimum_volumes,
    devs=deviations,
    upper=deviations+1.96*sds,
    lower=deviations-1.96*sds
  )
  
upper <- deviations+1.96*sds
lower <- deviations-1.96*sds  
  
   f <- ggplot(df) +

      geom_ribbon(aes(x=opt_vols, y=devs, ymin=lower, ymax=upper), fill="#56B4E9", alpha=0.5)+
      geom_line(aes(opt_vols, devs, color="Deviations"), size=1.5)+

      ggtitle(paste("Analog Direct Trial: ", substr(fn, nchar(fn)-20, nchar(fn)-4)))+
      xlab("Selected Optimum Volume [dB]")+
      ylab("Deviation from Max PESQ")+
      geom_hline(yintercept=0)+
      theme(legend.position="top") 
      
    f <- f + guides(color=guide_legend(title="Legend"), shape=FALSE)
  
  #show(f)
  plt_list[[fn]] <- f
  pdf.options(reset = TRUE, onefile = FALSE) 
}
ggarrange(plotlist=plt_list[1:10], ncol=2, nrow=5)
ggarrange(plotlist=plt_list[11:20], ncol=2, nrow=5)
ggarrange(plotlist=plt_list[21:30], ncol=2, nrow=5)
print(paste("Average Best Weight is:", mean(best_weights)))
print(paste("Best significant weight is:", mean(best_significant_weight)))
print(paste("Mean state diff weight is:", mean(stat_diff_weight)))
ggsave("ad_plot1.png", plot=(plt_list[[1]]+ggtitle("Title 1")), height=5, width=5)
ggsave("ad_plot2.png", plot=(plt_list[[2]]+ggtitle("Title 2")), height=5, width=5)
ad_pesq_deviation <- ggarrange(
  plt_list[[2]]+ggtitle("Analog Direct: All Weights Equivalent"), 
  plt_list[[1]]+ggtitle("Analog Direct: Higher Weights Not Equivalent"),
  nrow=1, ncol=2, common.legend=TRUE,
  legend="bottom"
  )
ggsave("ad_pesq_deviation.png", height=5, width=10)
```

### Discussion

One of the interesting things about this dataset is that it produces two
types of graphs reliably. The first type features a somewhat parabolic increase
at higher volumes. These graphs tend to have some points at the higher volumes
that don't have overlapping confidence intervals with the best data points.
These will have an optimal weighting value of less than 1. The second type of
graph is much flatter and is statistically equivalent across all points, so it has
an optimal weighting of 1.
This shows why the best significant value on average is around 0.881, and
not 1 as with P25 direct. For this technology,  include a bit of a buffer on 0.881, say 0.8. as a safety measure. Furthermore, given that
this is highest variance technology within the system under test, and the optimal weighting is
around .8, an overall weight of 0.8 system wide will work for 
producing a higher optimal volume.

# P25 Trunked Phase 2

## Metric 1: Verify with PESQ

```{r, echo = FALSE, warning = FALSE, message=FALSE}
test_files <- p25_phase_2_test_files

# create dataframe for populating
ov <- c()
fs <- c()
ps <- c()
fmd <- c()
pmd <- c()
fmv <- c()
pmv <- c()

pesq_opt_vol_unc <- c()
pesq_max_vol_unc <- c()

# get relevant data from each file
for(fn in test_files) {
  raw_data <- read_volume_csv(fn)
  opt_data <- raw_data[[1]]
  crv_data <- raw_data[[2]]
  proc_data <- summarise(
    group_by(crv_data, Volume_levels..dB.),
    FSF_avg=mean(FSF),
    PESQ_avg=mean(PESQ),
    FSF_var=var(FSF),
    PESQ_var=var(PESQ)
    )
  
  # interpolating polynomials for evaluating FSF and PESQ, linear interp
  faf <- approxfun(proc_data$Volume_levels..dB., proc_data$FSF_avg)
  paf <- approxfun(proc_data$Volume_levels..dB., proc_data$PESQ_avg)
  
  # add data
  ov <- c(ov, opt_data$Optimum)
  fs <- c(fs, faf(opt_data$Optimum))
  ps <- c(ps, paf(opt_data$Optimum))
  fmd <- c(fmd, max(proc_data$FSF_avg)-faf(opt_data$Optimum))
  pmd <- c(pmd, max(proc_data$PESQ_avg)-paf(opt_data$Optimum))
  fmv <- c(fmv, abs(proc_data[which.max(proc_data$FSF_avg),]$Volume_levels..dB. - ov[length(ov)]))
  pmv <- c(pmv, abs(proc_data[which.max(proc_data$PESQ_avg),]$Volume_levels..dB. - ov[length(ov)]))
  
  # propagate uncertainty through linear interpolation
  pesq_opt_vol_unc <- c(
    pesq_opt_vol_unc,
    prop_unc_lerp(proc_data$Volume_levels..dB., sqrt(proc_data$PESQ_var), opt_data$Optimum)
    )
  # get PESQ max volume level
  pesq_max_vol_level <- which.max(proc_data$PESQ_avg)
  pesq_max_vol_unc <- c(pesq_max_vol_unc, sqrt(proc_data[pesq_max_vol_level,]$PESQ_var))
}

comp_table <- data.frame(
  "Opt Volume"=ov,
  "FSF"=fs,
  "PESQ"=ps,
  "FSF Max Diff"=fmd,
  "PESQ Max Diff"=pmd,
  "FSF Max Vol Diff"=fmv,
  "PESQ Max Vol Diff"=pmv
)

# A few summary stats
df <- rbind(
  colMeans(comp_table),
  data.frame(lapply(comp_table, function(x) sqrt(var(x)/length(x))))
)

# Summary stats (min, max, mean and SE) for PESQ Max Diff column
PESQ_Max_Diff_stats_P25_Phase2 <- c(
  "Min" = min(pmd),
  "Max" = max(pmd),
  "Mean" = mean(pmd),
  "SE" = 0.0015779,
  "Lower" = mean(pmd) - 2*0.0015779,
  "Upper" = mean(pmd) + 2*0.0015779
 )

df[["Opt.Volume"]][[2]] = sqrt(df[["Opt.Volume"]][[2]]^2)
rownames(df) <- c("Mean", "SE")

headers <- c(
    "Opt Volume (dB)",
    "FSF",
    "PESQ",
    "FSF Max Diff",
    "PESQ Max Diff",
    "FSF Vol Diff (dB)",
    "PESQ Vol Diff (dB)"
    )

# make the table
knitr::kable(
  comp_table,
  col.names=headers
  )

knitr::kable(
  df,
  col.names=headers
  )
```

The table below gives the full uncertainty 
```{r, echo = FALSE, warning = FALSE, message=FALSE}
P25Phase2_t1 <- data.frame(
    "Opt_PESQ" = c(df$PESQ[[1]], sqrt(mean(pesq_opt_vol_unc^2))),
    "Max_PESQ" = c(df$PESQ[[1]] + df[["PESQ.Max.Diff"]][[1]], sqrt(mean(pesq_max_vol_unc^2)))
    )
row.names(P25Phase2_t1) <- c("Mean", "Standard Error")
knitr::kable(
  P25Phase2_t1
  )

```



## Metrics 2 and 3: Optimal interval and volume stability

```{r, echo = FALSE, warning = FALSE, message=FALSE, fig.align='center'}
ov <- c()
lb <- c()
ub <- c()

for(fn in test_files) {
  raw_data <- read_volume_csv(fn)
  opt_data <- raw_data[[1]]
  crv_data <- raw_data[[2]]
  proc_data <- summarise(
    group_by(
      crv_data,
      Volume_levels..dB.
      ),
    FSF_avg=mean(FSF),
    PESQ_avg=mean(PESQ),
    )

  ov <- c(ov, opt_data$Optimum)
  lb <- c(lb, opt_data$Lower_Interval)
  ub <- c(ub, opt_data$Upper_Interval)
}

# save data for later
p25_phase_2_lb <- lb
p25_phase_2_ub <- ub

opt_vals <- data.frame(
  "Optimum"=ov,
  "Lower_Bound"=lb, 
  "Upper_Bound"=ub,
  "Interval_Width"=(ub - lb)
)

ivrhist <- ggplot(opt_vals, aes(`Interval_Width`)) +
  geom_histogram(binwidth=.1) +
  xlab("Interval Width [dB]") +
  ylab("Counts") +
  ggtitle("Interval Width Distribution")

opthist <- ggplot(opt_vals, aes(`Optimum`)) +
  geom_histogram(binwidth=.1, fill="blue") +
  xlab("Optimum Volume [dB]") +
  ylab("Counts") +
  ggtitle("P25 Trunked Phase 2") + 
  theme(axis.text.x = element_text(face = "bold", size = 10)
  )
        
lbhist <- ggplot(opt_vals, aes(`Lower_Bound`)) +
  geom_histogram(binwidth=.1) +
  xlab("Lower Bound [dB]") +
  ylab("Counts") +
  ggtitle("Lower Bound Distribution")


ubhist <- ggplot(opt_vals, aes(`Upper_Bound`)) +
  geom_histogram(binwidth=.1) +
  xlab("Upper Bound [dB]") +
  ylab("Counts") +
  ggtitle("Upper Bound Distribution")

ggarrange(ivrhist, opthist, lbhist, ubhist, ncol=2, nrow=2)
ggsave("p25_phase_2_opt.png", plot=opthist, height=5, width=5)
p25_phase_2_opthist <- opthist
```

```{r, echo = FALSE, warning = FALSE, message=FALSE}
# uncertainty due to measurement
delta <- 1/sqrt(12)

# Summarize values
a <- summarise(
  opt_vals,
  Lower_Bound_Mean=mean(`Lower_Bound`),
  Lower_Bound_SE=sqrt(var(`Lower_Bound`)/length(`Lower_Bound`) + delta^2),
  Upper_Bound_Mean=mean(`Upper_Bound`),
  Upper_Bound_SE=sqrt(var(`Upper_Bound`)/length(`Upper_Bound`) + delta^2)
  )

knitr::kable(a)
```

## Uncertainty on bounds using coverage factor k= 2.26
```{r, echo = FALSE, warning = FALSE, message=FALSE}
k <- 2.26
Lower_Bound_SE <- sqrt(var(lb)/length(lb) + delta^2)
lower_bound_unc <- k * Lower_Bound_SE

Upper_Bound_SE <- sqrt(var(ub)/length(ub) + delta^2)
upper_bound_unc <- k* Upper_Bound_SE  
  
unc_bounds <-data.frame(lower_bound_unc, upper_bound_unc)

knitr::kable(unc_bounds, "pipe", col.names = c("Lower Bound Unc", "Upper Bound Unc"), align = c("l", "c", "c"))

```

## Weighting Analysis
Weighting analysis for P25 trunked Phase 2

```{r P25 trunked Phase 2 Interval Comparison, fig.height=11, fig.width=8, echo=FALSE}
# First: look at the range of weights to look at
weights <- seq(.5, .9, by=.01)
best_weights <- c()
best_significant_weight <- c()
stat_diff_weight <- c()
devs <- c()
plt_list <- list()

# grab data from only one of the tests for P25 direct
for(fn in test_files) {
  raw_data <- read_volume_csv(fn)
  opt_data <- raw_data[[1]]
  crv_data <- raw_data[[2]]
  proc_data <- summarise(
    group_by(
      crv_data,
      Volume_levels..dB.
      ),
    FSF_avg=mean(FSF),
    PESQ_avg=mean(PESQ),
    FSF_var=var(FSF),
    PESQ_var=var(PESQ)
    )
  
  paf <- approxfun(proc_data$Volume_levels..dB., proc_data$PESQ_avg)
  pesq_max <- max(proc_data$PESQ_avg)
  
  # propagate uncertainty through linear interpolation
  pesq_opt_vol_unc <- prop_unc_lerp(proc_data$Volume_levels..dB., sqrt(proc_data$PESQ_var), opt_data$Optimum)
  
  # get PESQ max volume level
  pesq_max_vol_level <- which.max(proc_data$PESQ_avg)
  pesq_max_vol_unc <- sqrt(proc_data[pesq_max_vol_level,]$PESQ_var)
  
  lb <- opt_data$Lower_Interval
  ub <- opt_data$Upper_Interval
  
  # figure out their corresponding optimal volumes
  optimum_volumes <- lb * (1-weights) + ub * weights
  
  # get PESQ scores at the newly made optimum volumes
  pesq_at_max_volume <- paf(optimum_volumes)

  # get deviations from max observed pesq
  deviations <- pesq_max - pesq_at_max_volume
  
  # standard deviation at each point
  sds <- sapply(optimum_volumes, function(x) prop_unc_lerp(proc_data$Volume_levels..dB., sqrt(proc_data$PESQ_var), x))
  sds <- sqrt(sds^2+pesq_max_vol_unc^2)/sqrt(40)
  
  # get best weight
  best_weights <- c(best_weights, weights[which.min(deviations)])
  
  # bind observations
  devs <- rbind(devs, deviations)
  
  # Find non-overlapping confidence interval from best weight
  min_dev_i <- which.min(deviations)
  tmp <- deviations - 1.96*sds - deviations[min_dev_i] - 1.96*sds[min_dev_i]
  ii <- which(tmp > 0)[1]
  if(is.na(ii)){
    best_significant_weight <- c(best_significant_weight, 1)
  } else {
    best_significant_weight <- c(best_significant_weight, weights[ii-1])
  }
  
  # Different from zero?
  sig_diff <- (deviations > 1.96*sds) | (deviations < -1.96*sds)
  if(any(sig_diff)){
    stat_diff_weight <- c(stat_diff_weight, weights[min(which(sig_diff))])
  } else {
    stat_diff_weight <- c(stat_diff_weight, 1)
  }
 
  # plot
  df <- data.frame(
    opt_vols=optimum_volumes,
    devs=deviations,
    upper=deviations+1.96*sds,
    lower=deviations-1.96*sds
  )
  f <- ggplot(df) +
    geom_point(aes(opt_vols, devs), color="blue")+
    geom_point(aes(opt_vols, upper), color="red")+
    geom_point(aes(opt_vols, lower), color="red")+
    ggtitle(paste("Test: ", fn))+
    xlab("Selected Optimum Volume (dB)")+
    ylab("PESQ Deviation from Max")
  
  #show(f)
  plt_list[[fn]] <- f
  
}
ggarrange(plotlist=plt_list, ncol=2, nrow=5)
print(paste("Average Best Weight is:", mean(best_weights)))
print(paste("Best significant weight is:", mean(best_significant_weight)))
print(paste("Mean stat diff weight is:", mean(stat_diff_weight)))


# group opthist graphic
all_opt_plots <- ggarrange(
  #plot_0, NULL,
  analog_direct_opthist,
  p25_phase_2_opthist,
  p25_direct_opthist,
  heights=c(3, 3, 3),
  widths=c(5, 5, 5),
  nrow=1,
  ncol=3
)

p <- annotate_figure(all_opt_plots, 
                top = text_grob("Optimal Volume Distribution", face = "bold", size = 12),
                bottom = NULL,
                left = NULL,
                right = NULL,
                fig.lab = NULL,
                fig.lab.pos = "top")


print(all_opt_plots)

ggsave("all_opt_plots.png", p, height=6, width=15)
ggexport(p, filename = "all_opt_plots_test.png", height = 400, width = 750)
```

### Discussion

The best significant weight is 1,
implying that the upper bound of the interval can be used, as with P25 direct.


# PESQ comparisons
## Compare the maximum observered PESQ to PESQ at optimal volume.

## PESQ Max Diff for each technology
```{r, echo = FALSE, warning = FALSE, message=FALSE, fig.align='center'}

library(knitr)

PESQ_Max_Diff_Stats <- data.frame(
  "P25 Direct" = PESQ_Max_Diff_stats_P25_Direct,
  "Analog Direct" = PESQ_Max_Diff_stats_Analog_Direct,
  "P25 Trunked Phase 2" = PESQ_Max_Diff_stats_P25_Phase2
  )
  
headers2 <- c(
  "P25 Direct",
  "Analog Direct",
  "P25 Trunked Phase 2"
  ) 
  
# make the table
knitr::kable(
  PESQ_Max_Diff_Stats,
  col.names=headers2,
  caption = "PESQ Max Diff Summary Stats"
  )
```

## 95% confidence intervals for the difference of maximum observed PESQ and estimated PESQ at optimal volume for each technology.

Estimate is obtained by interpolation.
Uncertainty is calculated as 2*sqrt(SE1^2 + SE2^2)

```{r, echo = FALSE, warning = FALSE, message=FALSE}

# 95CI for (Opt_PESQ - Max_PESQ)
CI_PESQ_Diffs <- data.frame(
  "Difference of Means" = c(abs(P25Direct_t1[1,1] - P25Direct_t1[1,2]),
                            abs(AnalogDirect_t1[1,1] - AnalogDirect_t1[1,2]),
                            abs(P25Phase2_t1[1,1] - P25Phase2_t1[1,2])),
  
  "SE1" = c(P25Direct_t1[2,1],
            AnalogDirect_t1[2,1],
            P25Phase2_t1[2,1]),
  
  "SE2" = c(P25Direct_t1[2,2],
            AnalogDirect_t1[2,2],
            P25Phase2_t1[2,2]),
  
  "Uncertainty" = c(2 * sqrt((P25Direct_t1[2,1])^2 + (P25Direct_t1[2,2])^2),
                    2 * sqrt((AnalogDirect_t1[2,1])^2 + (AnalogDirect_t1[2,2])^2),
                    2 * sqrt((P25Phase2_t1[2,1])^2 + (P25Phase2_t1[2,2])^2)),
  
  "Lower" = c(abs(P25Direct_t1[1,1] - P25Direct_t1[1,2]) - 2 * sqrt((P25Direct_t1[2,1])^2 + (P25Direct_t1[2,2])^2),
              abs(AnalogDirect_t1[1,1] - AnalogDirect_t1[1,2]) - 2 * sqrt((AnalogDirect_t1[2,1])^2 + (AnalogDirect_t1[2,2])^2),
              abs(P25Phase2_t1[1,1] - P25Phase2_t1[1,2]) - 2 * sqrt((P25Phase2_t1[2,1])^2 + (P25Phase2_t1[2,2])^2)),
  
  "Upper" = c(abs(P25Direct_t1[1,1] - P25Direct_t1[1,2]) + 2 * sqrt((P25Direct_t1[2,1])^2 + (P25Direct_t1[2,2])^2),
              abs(AnalogDirect_t1[1,1] - AnalogDirect_t1[1,2]) + 2 * sqrt((AnalogDirect_t1[2,1])^2 + (AnalogDirect_t1[2,2])^2),
              abs(P25Phase2_t1[1,1] - P25Phase2_t1[1,2]) + 2 * sqrt((P25Phase2_t1[2,1])^2 + (P25Phase2_t1[2,2])^2))
  ) 

row.names(CI_PESQ_Diffs) <- c("P25 Direct", "Analog Direct", "P25 Trunked Phase 2")
knitr::kable(
  CI_PESQ_Diffs
  )

CI_df <- data.frame(Technology = c("P25 Direct", "Analog Direct", "P25 Trunked Phase 2"),
                    
                    Diff_of_means = c(abs(P25Direct_t1[1,1] - P25Direct_t1[1,2]),
                            abs(AnalogDirect_t1[1,1] - AnalogDirect_t1[1,2]),
                            abs(P25Phase2_t1[1,1] - P25Phase2_t1[1,2])),
                    
                    lower_bounds = c(abs(P25Direct_t1[1,1] - P25Direct_t1[1,2]) - 2 * sqrt((P25Direct_t1[2,1])^2 + (P25Direct_t1[2,2])^2),
              abs(AnalogDirect_t1[1,1] - AnalogDirect_t1[1,2]) - 2 * sqrt((AnalogDirect_t1[2,1])^2 + (AnalogDirect_t1[2,2])^2),
              abs(P25Phase2_t1[1,1] - P25Phase2_t1[1,2]) - 2 * sqrt((P25Phase2_t1[2,1])^2 + (P25Phase2_t1[2,2])^2)),

                    upper_bounds = c(abs(P25Direct_t1[1,1] - P25Direct_t1[1,2]) + 2 * sqrt((P25Direct_t1[2,1])^2 + (P25Direct_t1[2,2])^2),
              abs(AnalogDirect_t1[1,1] - AnalogDirect_t1[1,2]) + 2 * sqrt((AnalogDirect_t1[2,1])^2 + (AnalogDirect_t1[2,2])^2),
              abs(P25Phase2_t1[1,1] - P25Phase2_t1[1,2]) + 2 * sqrt((P25Phase2_t1[2,1])^2 + (P25Phase2_t1[2,2])^2))
)

CI_PESQ_Diffs_plot <- ggplot(data = CI_df, aes(x = Technology)) + 
                        geom_errorbar(aes(ymin = lower_bounds,ymax = upper_bounds), size = 1.0)+
                        geom_point(aes(y = Diff_of_means), color = "blue", size = 5)+
                        geom_hline(yintercept = 0, linetype = "dashed", size = 2)+
                        xlab("\nTechnology")+
                        ylab("Difference of PESQ Means")+
                        ggtitle("95% Confidence Intervals for the Difference in Audio Quality Estimates and Observations")+
                        theme(
                          plot.title = element_text(hjust = 0.5),
                          axis.text.x = element_text(face = "bold", size = 12)
                          )

                        #theme_minimal()

#save an image of the plot
ggsave("ErrorBarPlot.PNG", 
       plot = CI_PESQ_Diffs_plot,
       width = 8,
       height = 6,
       units = "in")

print(CI_PESQ_Diffs_plot)

```

# Intertechnology Comparisons

In this section,the performance of the TVO
across the technologies within the system under test is evaluated. First, perform three different
permutation tests pairwise on the 3 technologies (3 choose 2) on the upper and
lower bound data. This is not performed on the optimum volume data as it is a
linear combination of upper and lower bound data, so all the relevant data
should be captured in those tests.

## P25 Direct vs. Analog Direct
```{r Permutation Test, include=FALSE, echo=FALSE}

skew <- function(x) {
  n <- length(x)
  m_3 <- sum((x-mean(x))^3)/n
  s <- sd(x)
  return(m_3/s^3)
}

permutation_comparisons <- function(x) {
  # Separate vector in half. This way, we cut computation time
  # in half since the rest of the datapoints are just the
  # negative
  l <- length(x)
  h1 <- x[1:(l/2)]
  
  h2 <- x[l:(l/2 + 1)]

  
  # get differences and compliment. Corresponds to adding the
  # mirrored half
  tmp <- h2 - h1
  tmp <- c(tmp, -tmp)
  return(tmp)
}

exact_permutation_test <- function(dataset_1, dataset_2) {

  # get lengths of the datasets
  N1 <- length(dataset_1)
  N2 <- length(dataset_2)

  # assert dataset sizes are equal
  stopifnot(N1 == N2)

  # pool data
  pooled_dataset <- c(dataset_1, dataset_2)

  # get all combinations of datapoints. Points reflected in the
  # vector are mutually exclusive, so c(a[k], a[-k]) == pooled
  combs <- combn(pooled_dataset, N1)

  # get mean of all permutations. Mean of all combinations
  # of N1 datapoints from the pooled data
  means <- colMeans(combs)
  variances <- apply(combs, 2, var)
  skews <- apply(combs, 2, skew)

  out <- list()

  out <- lapply(list('means'=means, 'vars'=variances, 'skews'=skews),
                permutation_comparisons)

  # mean differences
  return(out)
}

compare_mean_sep <- function(delta, size) {
  s <- c()
  for(ii in seq(0, size, by=delta)) {
    a <- rnorm(6, 0)
    b <- rnorm(6, ii)
    d <- exact_permutation_test(a, b)
    dmean <- mean(a) - mean(b)
    ex <- d[abs(d) >= abs(dmean)]
    s <- c(s, length(ex) / length(d))
  }
  return(s)
}
```

```{r Plot Generator, include=FALSE, echo=FALSE}

# Make a function out of this
generate_plots <- function(dataset1, dataset2, title) {
  # find observed mean difference and variance difference
  obs_mean_diff <-  abs(mean(dataset1)-mean(dataset2))
  obs_var_diff <- abs(var(dataset1)-var(dataset2))
  # obs_skew_diff <- abs(skew(dataset1)-skew(dataset2))
  obs_mean_diff
  obs_var_diff

  
  # perform the actual permutation test
  perm_test_data <- exact_permutation_test(dataset1, dataset2)
  
  # make it a dataframe, cause ggplot needs it
  perm_test_data_df <- data.frame(
    means=perm_test_data$means,
    vars=perm_test_data$vars
   
  )
  
  # upper and lower quantiles of the means
  bounds_mean <- quantile(perm_test_data$means, probs=c(.025, .975))
  bounds_var <- quantile(perm_test_data$var, probs=c(.025, .975))

    
  print(paste("obs mean: ", obs_mean_diff))
  print(paste("obs var: ", obs_var_diff))
  
  print(paste("mean bounds: ", bounds_mean))
  print(paste("var bounds: ", bounds_var))

  # plot
  ub_mean_hist <- ggplot(perm_test_data_df, aes(means)) +
    geom_histogram(bins=floor(sqrt(length(perm_test_data$means)))) +
    geom_vline(xintercept=bounds_mean[1], color='blue') +
    geom_vline(xintercept=bounds_mean[2], color='blue') + 
    geom_vline(xintercept=obs_mean_diff, color='red') +
    xlab(paste(title, "Mean Difference(dB)")) +
    ylab("Counts") +
    ggtitle(paste(title, "Mean Permutation Test"))
  
  ub_var_hist <- ggplot(perm_test_data_df, aes(vars)) +
    geom_histogram(bins=floor(sqrt(length(perm_test_data$means)))) +
    geom_vline(xintercept=bounds_var[1], color='blue') +
    geom_vline(xintercept=bounds_var[2], color='blue') + 
    geom_vline(xintercept=obs_var_diff, color='red') +
    xlab(paste(title, "Variance Difference(dB)")) +
    ylab("Counts") +
    ggtitle(paste(title, "Variance Permutation Test"))
  
  ggarrange(ub_mean_hist, ub_var_hist, nrow=2)

}

```

```{r P25 Direct vs. Analog Direct}

```

Discussion: Here is a permutation test for the mean
difference and variance difference between the upper bound data for the two data
sets. If the red line is outside of the blue lines, we have
a 95% level of confidence that the two data sets have different mean or variance
respectively. The red line is inside the blue lines, for the mean,
but the variance is on the 95% line, which is probably due to the low number of
unique values. The algorithm performs well at determining the upper bound as the transmit volume settings begin to induce clipping. In conclusion,  there is a good chance
the distribution of the upper bound between P25 direct and analog direct are
statistically the same, but difficult to say.

The lower bound, however, has both firmly outside the 95% percent bounds. This
means that the lower bound distributions are likely not the same in terms
of the distribution.

## P25 Direct vs. P25 Trunked Phase 2
```{r P25 Direct vs. P25 Phase 2}
generate_plots(p25_direct_ub, p25_phase_2_ub, "Upper Bound")
generate_plots(p25_direct_lb, p25_phase_2_lb, "Lower Bound")
```

Discussion:
For the upper bound, both mean and variance are firmly in the bounds. This gives
confidence that the distributions are the same.

The lower bounds are inconclusive since both the mean and variance test lay on
the 95% bound, leaning towards saying the lower
bound distributions are different.

## Analog Direct vs. P25 Trunked Phase 2
```{r Analog Direct vs. P25 Trunked Phase 2}

```

Discussion:
The upper bound distribution is the same.
The lower bound is firmly in the realm of being different.

## Conclusions
The upper bounds between the technologies are all likely to be pulled from
the same distribution. The lower bounds are likely not. That said, the upper
bound uniformity between all the technologies is arguably most important as it
demarks a clear transition to lower FSF and PESQ scores as the transmit volume begins to cause clipping.

# Variance Based on Volume Levels
This section studies and provides a brief analysis of how the variance in the
TVO data changes with respect to the transmit volume level.

## P25 Direct
```{r, echo=FALSE, warning=FALSE, fig.height=11, fig.width=8}
test_files <- p25_direct_test_files

# vector to store plots
plt_list <- list()

# vector for average values
avg_fsf_std <- c()

# get relevant data from each file
for(fn in test_files) {
  raw_data <- read_volume_csv(fn)
  crv_data <- raw_data[[2]]
  proc_data <- summarise(group_by(crv_data, Volume_levels..dB.), FSF_var=var(FSF), PESQ_var=var(PESQ))
  
  plt <- ggplot(data=proc_data) +
    geom_point(aes(Volume_levels..dB., FSF_var, color="blue")) +
    geom_line(aes(Volume_levels..dB., FSF_var, color="blue")) +
 
    xlab("Volume Level (dB)") + ylab("Score Variance") +
    labs(color="Score Type/n") +
    scale_color_manual(labels=c("FSF", "PESQ"), values=c("red", "blue")) +
    theme(legend.position="top")
  
  # print(plt)
  avg_fsf_std <- c(avg_fsf_std, mean(sqrt(proc_data$FSF_var)))
  
  # add to plotlist
  plt_list[[fn]] <- plt
}

FSF_var_P25direct_fname <- test_files[6]
print(mean(avg_fsf_std))
ggarrange(plotlist=plt_list, ncol=2, nrow=5)

FSF_var_P25direct <- plt_list[[6]]
```
Here we discuss the common trends over the 10 volume adjust tests. There is a
significant increase in the variance of PESQ scores at 0 dB (maximum volume). In FSF, the variance is stable in the -40 dB to -10 dB range, and a significant increase around -5 dB
followed by a slight increase. 

## Analog Direct
```{r, echo=FALSE, warning=FALSE, fig.height=11, fig.width=8}
test_files <- analog_direct_test_files

# vector to store plots
plt_list <- list()

avg_fsf_std <- c()

# get relevant data from each file
for(fn in test_files) {
  raw_data <- read_volume_csv(fn)
  crv_data <- raw_data[[2]]
  proc_data <- summarise(group_by(crv_data, Volume_levels..dB.), FSF_var=var(FSF), PESQ_var=var(PESQ))
  
  plt <- ggplot(data=proc_data) +
    geom_point(aes(Volume_levels..dB., FSF_var, color="blue")) +
    geom_line(aes(Volume_levels..dB., FSF_var, color="blue")) +

    xlab("Volume Level (dB)") + ylab("Score Variance") +
    labs(color="Score Type\n") +
    scale_color_manual(labels=c("FSF", "PESQ"), values=c("red", "blue")) +
    theme(legend.position="top")
  
  # add to plotlist
  plt_list[[fn]] <- plt
  avg_fsf_std <- c(avg_fsf_std, mean(sqrt(proc_data$FSF_var)))
}

print(mean(avg_fsf_std))
ggarrange(plotlist=plt_list[1:10], ncol=2, nrow=5)

```

```{r, echo=FALSE, warning=FALSE, fig.height=11, fig.width=8}

```

```{r, echo=FALSE, warning=FALSE, fig.height=11, fig.width=8}
FSF_var_AnalogDirect_fname <- test_files[24]

FSF_var_AnalogDirect  <- plt_list[[24]]

```

PESQ and FSF both have a stable, slight increase in variance from -40 dB to around -15 dB. For
PESQ, a slight dip followed by a large spike occurs. FSF continues increase at a
slightly faster rate. 

## P25 Trunked Phase 2
```{r, echo=FALSE, warning=FALSE, results=FALSE, fig.height=11, fig.width=8}
test_files <- p25_phase_2_test_files

# vector to store plots
plt_list <- list()

avg_fsf_std <- c()

# get relevant data from each file
for(fn in test_files) {
  raw_data <- read_volume_csv(fn)
  crv_data <- raw_data[[2]]
  proc_data <- summarise(group_by(crv_data, Volume_levels..dB.), FSF_var=var(FSF), PESQ_var=var(PESQ))
  
  plt <- ggplot(data=proc_data) +
    geom_point(aes(Volume_levels..dB., FSF_var, color="blue")) +
    geom_line(aes(Volume_levels..dB., FSF_var, color="blue")) +

    xlab("Volume Level (dB)") + ylab("Score Variance") +
    labs(color="Score Type\n") +
    scale_color_manual(labels=c("FSF", "PESQ"), values=c("red", "blue")) +
    theme(legend.position="top")

  
  # add to plotlist
  plt_list[[fn]] <- plt
  
}

FSF_var_P25phase2_fname <- test_files[5]

FSF_var_P25phase2 <- plt_list[[5]]

ggarrange(plotlist=plt_list, ncol=2, nrow=5)

```

```{r}
print(avg_fsf_std <- c(avg_fsf_std, mean(sqrt(proc_data$FSF_var))))
```

```{r, echo=FALSE, warning=FALSE, results=FALSE, fig.height=11, fig.width=8}
# creating plot that compares variance for all techs

# P25 direct info for plot BLUE
fn <- FSF_var_P25direct_fname
raw_data <- read_volume_csv(fn)
crv_data <- raw_data[[2]]
pd_proc_data <- summarise(group_by(crv_data, Volume_levels..dB.), FSF_var=var(FSF), PESQ_var=var(PESQ))
pd_proc_data$Technology <- "P25 Direct"

# Analog direct info for plot RED
fn <- FSF_var_AnalogDirect_fname
raw_data <- read_volume_csv(fn)
crv_data <- raw_data[[2]]
ad_proc_data <- summarise(group_by(crv_data, Volume_levels..dB.), FSF_var=var(FSF), PESQ_var=var(PESQ))
ad_proc_data$Technology <- "Analog Direct"

# P25 trunked Phase 2 info for plot GREEN
fn <- FSF_var_P25phase2_fname
raw_data <- read_volume_csv(fn)
crv_data <- raw_data[[2]]
p2_proc_data <- summarise(group_by(crv_data, Volume_levels..dB.), FSF_var=var(FSF), PESQ_var=var(PESQ))
p2_proc_data$Technology <- "P25 Trunked Phase 2"

proc_data <- bind_rows(pd_proc_data, ad_proc_data, p2_proc_data)

# all three FSF variance plots on one graph
plt <- ggplot(data=proc_data) +
  geom_point(aes(Volume_levels..dB., FSF_var, color= Technology),alpha = 1, size = 4) +
  geom_line(aes(Volume_levels..dB., FSF_var, color= Technology),alpha = 1, size = 1.5) +

  xlab("Volume [dB]") + ylab("FSF Score Variance") +
  labs(color="Technology Type\n") +

  theme(legend.position="top")
new_plt <- plt + scale_color_brewer(palette = "Dark2")

# Let's see the plot
ggarrange(new_plt)

#save an image of the plot
ggsave("FSF Variance Plots.png", 
       plot = new_plt,
       width = 8,
       height = 6,
       units = "in")

# three separate FSF variance plots next to each other
var_plots <- c(FSF_var_P25direct, FSF_var_AnalogDirect, FSF_var_P25phase2)

var_plots <- ggarrange(FSF_var_P25direct, FSF_var_AnalogDirect, FSF_var_P25phase2, ncol = 1, nrow = 3)

print(mean(avg_fsf_std))

```
FSF is stable from -40 dB to ~-10 dB.


# Conclusions
Here are some of the main takeaways from this analysis:

The TVO is stable across technologies.


From metrics 2 & 3, the upper bound reliably catches the transition point of the curve where the transmit volume begins to induce clipping and is quite low variance across the different
technologies. The lower bound has higher variance across technologies, but since it is possible to weight the upper bound more heavily, the optimum volume is well anchored.

From the weighting analysis, all technologies aside from analog direct have statistically equivalent PESQ scores on the detected interval. And
for analog direct, the data suggests that it is reasonable to increase the weight to 0.80 with no statistical change in speech quality. This is advantageous because it allows a higher optimum volume level.

From the variance analysis, the level of noise in the data
collected is uniform across all volume levels until the transmit volume approaches higher volume levels that begin to cause clipping. 
